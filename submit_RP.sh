#!/bin/bash
#PBS -N Fraud_GNN_IBM_Medium
#PBS -l select=1:ncpus=8:mem=32GB:ngpus=1:Qlist=ee:host=comp055
#PBS -l walltime=24:00:00
#PBS -j oe
#PBS -o output.out
#PBS -m ae
#PBS -M 23724617@sun.ac.za
#PBS -V

set -euxo pipefail

umask 0077
SCRATCH_BASE="/scratch-small-local"
[ -d "${SCRATCH_BASE}" ] || SCRATCH_BASE="$HOME/scratch"
TMP="${SCRATCH_BASE}/${PBS_JOBID//./-}"
mkdir -p "${TMP}"
echo "Temporary work dir: ${TMP}"


cd ${TMP}

cleanup() {
  echo "Copying results back to ${PBS_O_WORKDIR}/ (cleanup)"
  /usr/bin/rsync -vax --progress \
    --include '/csv_results/***' \
    --include '/optimization_results.db' \
    --include '/output.out' \
    --include '/worker*.log' \
    --exclude '*' \
    "${TMP}/" "${PBS_O_WORKDIR}/" || true
  [ "$?" -eq 0 ] && /bin/rm -rf "${TMP}"
}
trap cleanup EXIT

echo "Copying from ${PBS_O_WORKDIR}/ to ${TMP}/"
/usr/bin/rsync -vax --delete "${PBS_O_WORKDIR}/" "${TMP}/"
cd "${TMP}"

command -v nvidia-smi >/dev/null 2>&1 && nvidia-smi || true

# prebuilt env (extract into its own subdir so activate path exists)
mkdir -p "${TMP}/RP_env"
tar -xzf "$PBS_O_WORKDIR/RP_env.tar.gz" -C "${TMP}/RP_env"
# conda-pack activate references some unset vars under set -u; relax then restore
set +u
source "${TMP}/RP_env/bin/activate"
command -v conda-unpack >/dev/null 2>&1 && conda-unpack || true
set -u

# threads consistent with ncpus=8
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export QT_QPA_PLATFORM=offscreen
export MPLCONFIGDIR="${TMP}/.mpl"
mkdir -p "${MPLCONFIGDIR}"

python -c "import torch, sys; print('torch', torch.__version__, 'cuda', getattr(torch.version,'cuda',None), 'cuda_available', torch.cuda.is_available())"

# Get dataset from first argument
if [ -z "${1:-}" ]; then
    echo "ERROR: No dataset argument provided."
    echo "Usage: qsub -F \"DATASET_NAME\" submit_RP.sh"
    exit 1
fi
DATASET_NAME="$1"

if [[ -f train.py ]]; then
  echo "Starting Training on GPU 0 for dataset: $DATASET_NAME"
  
  # Run directly in foreground (no need for background & wait for single job)
  CUDA_VISIBLE_DEVICES=0 python -u train.py "$DATASET_NAME" > "worker_${DATASET_NAME}.log" 2>&1
  
else
  echo "ERROR: missing training script"; ls -lah; exit 2
fi

echo "DONE"
